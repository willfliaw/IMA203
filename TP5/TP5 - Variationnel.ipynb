{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William Liaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débruitage par régularisation quadratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment utiliser l'outil `resoud_quad_fourier` pour trouver le minimiseur de cette énergie (voir le programme `minimisation_quadratique`)?\n",
    "\n",
    "On cherche à retrouver u comme minimiseur de l'énergie:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\|u-v\\|^2 + \\lambda\\|\\nabla u\\|^2 \\\\\n",
    "&\\|\\delta*u-v\\|^2 + \\lambda(\\|K_x * u\\|^2 + \\|K_y * u\\|^2) \\\\\n",
    "&\\|\\delta*u-v\\|^2 + \\|\\sqrt{\\lambda}K_y * u-0\\|^2 + \\|\\sqrt{\\lambda}K_x * u-0\\|^2 \\\\\n",
    "&\\|\\sqrt{\\lambda}K_x * u-0\\|^2 + \\|\\sqrt{\\lambda}K_y * u-0\\|^2 + \\|\\delta*u-v\\|^2 \\\\\n",
    "&\\sum\\limits_{i=1}^3 \\|K_i * u - V_i\\|^2\n",
    "\\end{align*}\n",
    "\n",
    "Ainsi, minimiser l'expression originale de l'énergie revient à trouver une image qui minimise $\\sum\\limits_{i=1}^3 \\|K_i * u - V_i\\|^2$, pour $K = (\\sqrt{\\lambda}K_x, \\sqrt{\\lambda}K_y, \\delta)$ et $V = (0, 0, v)$. En prenant la transformée de Fourier de la dernière expression, on obtient: $\\sum\\limits_{\\omega\\in f}\\sum\\limits_{i=1}^3 |\\hat{K}_i \\cdot \\hat{u} - \\hat{V}_i|^2 \\Rightarrow \\sum\\limits_{i=1}^3\\sum\\limits_{\\omega\\in f} |\\hat{K}_i \\cdot \\hat{u} - \\hat{V}_i|^2$. En analysant le terme dans la sommation:\n",
    "\n",
    "\\begin{align*}\n",
    "&|\\hat{K}_i \\cdot \\hat{u} - \\hat{V}_i|^2 \\\\\n",
    "&|\\hat{K}_i|^2 \\cdot |\\hat{u}|^2 - 2\\mathbb{R}(\\hat{V}_i\\overline{\\hat{K}_i}\\overline{\\hat{u}}) + |\\hat{V}_i|^2 \\\\\n",
    "&|\\hat{K}_i|^2\\bigg(|\\hat{u}|^2 - 2\\mathbb{R}\\bigg(\\frac{\\hat{V}_i\\overline{\\hat{K}_i}\\overline{\\hat{u}}}{|\\hat{K}_i|^2}\\bigg) + \\frac{|\\hat{V}_i|^2}{|\\hat{K}_i|^2}\\bigg) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "En supprimant les termes indépendants de $u$, on peut voir que la minimisation de l'expression originale de l'énergie revient à minimiser: $|\\hat{u}|^2 - 2\\mathbb{R}\\bigg(\\frac{\\hat{V}_i\\overline{\\hat{K}_i}}{|\\hat{K}_i|^2}\\overline{\\hat{u}}\\bigg) + |\\gamma|^2=|\\hat{u}-\\gamma|^2, \\quad \\overline{\\gamma} = \\frac{\\hat{V}_i\\overline{\\hat{K}_i}}{|\\hat{K}_i|^2}$. Ainsi:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\hat{u_{\\min}} = \\gamma \\\\\n",
    "\\therefore\\ & u_{\\min} = \\mathbb{R}\\bigg(\\mathscr{F^{-1}}\\bigg(\\frac{\\hat{V}_i\\overline{\\hat{K}_i}}{|\\hat{K}_i|^2}\\bigg)\\bigg)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décrire le résultat de ce débruitage lorsque $\\lambda$ est très grand ou très petit.\n",
    "\n",
    "Lorsque la valeur de $\\lambda$ est très petite, le processus de débruitage donne des résultats peu significatifs: l'image reste bruitée. À mesure que nous accroissons la valeur de $\\lambda$, la qualité de l'image restaurée se rapproche de celle de l'image parfaite, représentée par $u$. Cependant, en continuant à augmenter $\\lambda$, l'image obtenue devient de plus en plus floue.\n",
    "\n",
    "L'augmentation de $\\lambda$ correspond à un renforcement du poids du terme de régularisation. Cela se traduit par une préférence croissante pour une image résultante présentant des caractéristiques statistiques naturelles, plutôt que de rester strictement fidèle aux données d'origine. En d'autres termes, une pondération plus importante de ce terme favorise une régularité visuelle accrue au détriment de la fidélité stricte aux données initiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Après avoir ajouté un bruit d'écart type $\\sigma = 5$ à l'image de lena, trouver (par dichotomie) le paramètre $\\lambda$ pour lequel $\\|\\tilde{u} − v\\|^2 \\sim \\|u − v\\|^2$. C'est-à-dire le paramètre pour lequel l'image reconstruite $\\tilde{u}$ est à la même distance de l'image parfaite $u$ que ne l'est l'image dégradée.\n",
    "\n",
    "Par le biais de la méthode de dichotomie, notre démarche a débuté en définissant un intervalle initial de λ de manière suffisamment étendue. À chaque itération, nous avons calculé la différence $\\|\\tilde{u} − v\\|^2 - \\|u − v\\|^2$ pour les valeurs de $\\lambda$: $\\lambda_{\\text{left}}$ et $\\lambda_{\\text{right}}$, ainsi qu'à la moyenne entre ces deux valeurs. Ensuite, nous avons remplacé l'une des valeurs de $\\lambda_{\\text{left}}$ et $\\lambda_{\\text{right}}$ par cette moyenne. Grâce à cet algorithme itératif, nous avons pu déterminer que la valeur optimale de $\\lambda$ est égale à $3.207$. Cela suggère que, dans le contexte d'optimisation, cette valeur spécifique de $\\lambda$ minimise la différence entre les images $\\tilde{u}$ et $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomic_search_zero(\n",
    "    function, left_bound, right_bound, tolerance=1e-6, max_iterations=1000\n",
    "):\n",
    "    for iteration in range(max_iterations):\n",
    "        mid_point = (left_bound + right_bound) / 2\n",
    "\n",
    "        # Evaluate the function at the mid-point and its neighbors\n",
    "        f_mid = function(mid_point)\n",
    "        f_left = function(left_bound)\n",
    "        f_right = function(right_bound)\n",
    "\n",
    "        # Check if the minimum is on the left or right side of the interval\n",
    "        if f_left * f_mid <= 0:\n",
    "            right_bound = mid_point\n",
    "        else:\n",
    "            left_bound = mid_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(f_right - f_left) < tolerance:\n",
    "            break\n",
    "\n",
    "    # Return the minimum value and the argument at which it occurs\n",
    "    min_value = function((left_bound + right_bound) / 2)\n",
    "    min_argument = (left_bound + right_bound) / 2\n",
    "    return min_value, min_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread(\"lena.tif\")\n",
    "imb = degrade_image(im, 5**2)\n",
    "\n",
    "t = norm2(im - imb) ** 2\n",
    "function = lambda lamb: norm2(minimisation_quadratique(imb, lamb) - imb) ** 2 - t\n",
    "\n",
    "min_error, min_lambda = dichotomic_search_zero(\n",
    "    function=function,\n",
    "    left_bound=1,\n",
    "    right_bound=1e3,\n",
    "    tolerance=1e-6,\n",
    "    max_iterations=1000,\n",
    ")\n",
    "\n",
    "print(f\"Best lambda: {min_lambda:.3f}\")\n",
    "viewimage(minimisation_quadratique(imb, min_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Écrire un algorithme pour trouver le paramètre $\\lambda$ tel que $\\|\\tilde{u} − u\\|^2$ soit minimale. (dans le cadre de ce TP on connaît l'image parfaite $u$). Commentaires?\n",
    "\n",
    "Une fois de plus, par le biais de la méthode de dichotomie, notre démarche a débuté en définissant un intervalle initial de λ de manière suffisamment étendue. À chaque itération, nous avons calculé la différence $\\|\\tilde{u} − u\\|^2$ pour les valeurs de $\\lambda$: $\\lambda_{\\text{left}}$ et $\\lambda_{\\text{right}}$, ainsi qu'à la moyenne entre ces deux valeurs. Ensuite, nous avons remplacé l'une des valeurs de $\\lambda_{\\text{left}}$ et $\\lambda_{\\text{right}}$ par cette moyenne. Grâce à cet algorithme itératif, nous avons pu déterminer que la valeur optimale de $\\lambda$ est égale à $1.122$. Cela suggère que, dans le contexte d'optimisation, cette valeur spécifique de $\\lambda$ minimise la différence entre les images $\\tilde{u}$ et $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomic_minimization(\n",
    "    function, left_bound, right_bound, tolerance=1e-6, max_iterations=1000\n",
    "):\n",
    "    x = []\n",
    "    y = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mid_point = (left_bound + right_bound) / 2\n",
    "\n",
    "        # Evaluate the function at the mid-point and its neighbors\n",
    "        f_mid = function(mid_point)\n",
    "        f_left = function(left_bound)\n",
    "        f_right = function(right_bound)\n",
    "        if f_right <= f_mid:\n",
    "            left_bound = mid_point\n",
    "        else:\n",
    "            right_bound = mid_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(f_right - f_left) < tolerance:\n",
    "            break\n",
    "\n",
    "    # Return the minimum value and the argument at which it occurs\n",
    "    min_value = function((left_bound + right_bound) / 2)\n",
    "    min_argument = (left_bound + right_bound) / 2\n",
    "    return min_value, min_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread(\"lena.tif\")\n",
    "imb = degrade_image(im, 5**2)\n",
    "\n",
    "function = lambda lamb: norm2(minimisation_quadratique(imb, lamb) - im) ** 2\n",
    "\n",
    "min_error, min_lambda = dichotomic_minimization(\n",
    "    function=function,\n",
    "    left_bound=1,\n",
    "    right_bound=1e3,\n",
    "    tolerance=1e-6,\n",
    "    max_iterations=1000,\n",
    ")\n",
    "\n",
    "print(f\"Best lambda: {min_lambda:.3f}\")\n",
    "viewimage(minimisation_quadratique(imb, min_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse des résultats révèle que la deuxième méthode offre une image améliorée et une erreur plus réduite $\\|\\tilde{u} − u\\|^2$. D'un point de vue géométrique, on peut expliquer cette différence: la première méthode recherche l'intersection entre les cercles de rayon $\\sigma^2$ centrés en $u$ et $v$, tandis que la deuxième méthode trace $\\tilde{u} = v(\\lambda)$, une ligne à partir de $v$, et trouve la projection orthogonale de $u$. Cette approche semble mieux capturer la structure géométrique de l'espace des solutions, conduisant ainsi à des résultats plus précis et à une réduction de l'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débruitage par variation totale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread(\"lena.tif\")\n",
    "imb = degrade_image(im, 5**2)\n",
    "\n",
    "u, energ01 = minimise_TV_gradient(imb, 3, 0.1, 100)\n",
    "u, energ02 = minimise_TV_gradient(imb, 3, 0.2, 100)\n",
    "u, energ04 = minimise_TV_gradient(imb, 3, 0.4, 100)\n",
    "u, energ06 = minimise_TV_gradient(imb, 3, 0.6, 100)\n",
    "u, energ08 = minimise_TV_gradient(imb, 3, 0.8, 100)\n",
    "u, energ10 = minimise_TV_gradient(imb, 3, 1.0, 100)\n",
    "\n",
    "plt.plot(np.log(energ01), label=\"0.1\")\n",
    "plt.plot(np.log(energ02), label=\"0.2\")\n",
    "plt.plot(np.log(energ04), label=\"0.4\")\n",
    "plt.plot(np.log(energ06), label=\"0.6\")\n",
    "plt.plot(np.log(energ08), label=\"0.8\")\n",
    "plt.plot(np.log(energ10), label=\"1.0\")\n",
    "\n",
    "plt.gca().spines[\"top\"].set_alpha(0.0)\n",
    "plt.gca().spines[\"bottom\"].set_alpha(0.3)\n",
    "plt.gca().spines[\"right\"].set_alpha(0.0)\n",
    "plt.gca().spines[\"left\"].set_alpha(0.3)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est évident que nous n'atteignons toujours pas le même minimum d'énergie. Nous faisons face à des problèmes numériques lors de la minimisation de la variation totale par descente de gradient à pas constant. Plus spécifiquement, lorsque nous optons pour une valeur de pas très grande, l'algorithme rencontre des difficultés à converger vers le minimum recherché. D'un autre côté, une taille de pas excessivement petite entraîne une convergence plus lente de l'algorithme.\n",
    "\n",
    "Il est d'une importance primordiale de bien comprendre le problème en question afin de choisir une taille de pas optimale: ni trop grande pour garantir la convergence de l'algorithme, ni trop petite pour permettre à l'algorithme de tirer le meilleur parti de chaque itération. Trouver le bon équilibre dans le choix de la taille du pas est essentiel pour assurer une convergence stable et efficace du processus d'optimisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Chambolle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread(\"lena.tif\")\n",
    "imb = degrade_image(im, 5**2)\n",
    "lamb = 3\n",
    "\n",
    "u_grad, energ_quad = minimise_TV_gradient(imb, lamb, 0.6, 100)\n",
    "u_chamb = vartotale_Chambolle(imb, lamb, 100)\n",
    "\n",
    "function = lambda u: norm2(u - im) ** 2\n",
    "print(function(u_grad))\n",
    "print(function(u_chamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que les deux méthodes donnent des résultats similaires en comparant $\\|\\tilde{u}_{\\text{grad}} - v\\|^2$ et $\\|\\tilde{u}_{\\text{chamb}} - v\\|^2$. Cependant, il est à noter que la méthode de Chambolle présente une efficacité computationnelle nettement plus rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread(\"lena.tif\")\n",
    "imb = degrade_image(im, 5**2)\n",
    "\n",
    "errvt = []\n",
    "erreur = []\n",
    "vk = np.concatenate((np.linspace(0.1, 3, 50), np.linspace(3, 60, 120)))\n",
    "\n",
    "for k in vk:\n",
    "    restq = minimisation_quadratique(imb, k)\n",
    "    restva = vartotale_Chambolle(imb, k)\n",
    "    erreur.append(norm2(im - restq))\n",
    "    errvt.append(norm2(restva - im))\n",
    "\n",
    "plt.plot(vk, erreur, label=\"Quadratique\")\n",
    "plt.plot(vk, errvt, label=\"Chambolle\")\n",
    "\n",
    "plt.gca().spines[\"top\"].set_alpha(0.0)\n",
    "plt.gca().spines[\"bottom\"].set_alpha(0.3)\n",
    "plt.gca().spines[\"right\"].set_alpha(0.0)\n",
    "plt.gca().spines[\"left\"].set_alpha(0.3)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(vk[np.argmin(erreur)])\n",
    "print(vk[np.argmin(errvt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les paramètres optimaux trouvés pour la minimisation quadratique étaient $\\lambda = 1.165$, et pour la variation totale $\\lambda = 41.319$.\n",
    "\n",
    "En conclusion, il semble que, du point de vue quantitatif, la méthode de variation totale soit plus efficace. En premier lieu, elle s'avère au moins aussi rapide à calculer que la méthode de minimisation quadratique par la méthode de Chambolle, évitant ainsi le besoin de calcul des transformées de Fourier. En outre, l'erreur obtenue par cette méthode est $1.199$ fois plus petite que celle obtenue par l'autre approche.\n",
    "\n",
    "D'un point de vue qualitatif, les résultats révèlent une nuance plus subtile. Bien que la méthode de variation totale réduise de manière évidente le bruit dans l'image, il semble que nous ayons sacrifié un peu de contraste (même si cela ne se manifeste pas clairement sur les histogrammes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
